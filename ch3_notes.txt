Chapter 3: Linear Regression

Consider advertising data set

Questions:
    1. Is there a relationship between advertising budget and sales
    2. How strong is the reationship in 1.
    3. Which media are associated with sales?
    4. How strong are the relationships in 3.
    5. How accurately can we predict future sales?
    6. Are the relationsps linear?
    7. Is there synergy
        i.e. do the media interact?


3.1 Simple Linear Regression:
    Assume
        Y about= B0 + B1*X
    
    We could let X = TV advertising and Y = sales

    Sales about= B0 + B1*TV

    y = sales = [22.1]
                [10.4]
                [....]
                [13.4]

    x = TV = [230.1]
             [44.50]
             [.....]
             [232.1]

    Assumption yi about= B0 + B1*xi
                |
                v
    22.1 about= B0 + B1*230.1
    10.4 about= B0 + B1*44.5
            .....
    13.4 about= B0 + B1*232.1

    Continue the above in vector form:
      y about= B0 1 + B1*x
    [22.1]     [1]        [230.1]
    [10.4] = B [1] + B1 * [44.50]
    [....]     [.]        [.....]
    [13.4]     [1]        [232.1]

                |
                v
    y = [1  x] [B0]
               [B1]

    [1  230.1] [B0]
    [1  44.50] [B1]
    [.  .....]
    [1  232.1]


    Define y_hat = X*B_hat
                 = [1  X] [B0_hat]
                          [B1_hat]
        
        y = B0_hat*1 + B1_hat*x

        plots Least Squares Fit

    These are the formulas for B0_hat and B1_hat

    B1_hat = sum(n, i=1, (xi - x_hat)(yi - y_hat))
           = sum(n, i=1, (xi - x)^2)

    B0_hat = y_hat - B1_hat*x_hat
        
        where x_hat = 1/2*sum(n, i=1, xi) and y_hat = 1/n*sum(n, i=1, yi)


    Figure 3.1
        Let 
            f(B0, B1) = sum(n, i=1, yi - (B0+B1*xi))^2

        Plot (B0, B1, f(B0, B1))
            B0_hat and B1_hat is the minimum of f



    How confident am I in the values of B0_hat and B1_hat?
        Assume 
            Y = B0 + B1*x + epsilon, where x and epsilon are independent
        

        Toy example: 
            B0 = 2, B1 = 3
            Data Y = 2 + 3x + epsilon

            Generate synthetic data
                        [x001]
                    X = [....]
                        [x100]

                        [epsilon001]
                    Y = [..........]
                        [epsilon100]

            yi = 2 = 3*xi + epsilon

            Pretend we don't kow B0 and B1

            Solve y = X*B:

                [y001]   [1  x001] [B0]
                [....] = [.  ....] [B1]
                [y100]   [1  x100]

                see Fig. 3.3
                    Left
                        red is true model
                            B0=2, B1=3
                        
                        blue is least squares Fit
                            B0_hat, B1_hat

                    Right:
                        repeat 10 times to get different B0_hat, B1_hat 

                B0_hat, B1_hat are random variables because X and
                 epsilon are random variables
                
        
            Thought experment:
                1. Generate obsevations x1, ..., xn
                        yi = 2 + 3*xi + epsiloni

                2. Estimate B0_hat, B1_hat, using least squares to get our
                    fit y = B0_hat + B1_hat*X
                
                3. Do this many times. Then 
                    B0 will average to 2
                    B1 will average to 3
                    B0_hat, B1_hat are unbiased estimates



    Synthetic example: generate data()
        yi = 2 + 3xi +epsiloni

        xi = randomly genereated points
        epsiloni = normally distributed with 0 mean

        Data: {(xi, yi)}_i=1 ^100      Fig. 3.3

            Plot of true model: y = 2+3xi

            Use least squares to fit y = B0 + B1x to {(xi, yi)}_i=1 ^100
             Givinig B0_hat, B1_hat
              y = B0_hat + B1_hat8x

            Generate another data set {(xi, yi)}_i=1 ^100 
             Use different B0_hat, B1_hat
                repeat this 10 times
                    we get plot on the right in Fig. 3.3
        
                How much do B0_hat and B1_hat vary?????

                    Consider random observations y1, ..., ya from Y
                     Then we can estimate the mean mew of Y by
                        mew_hat = 1/n (sum, n, i=1, yi)                

                        mew_hat is unbiased: E(mew_hat) = mew

                        Var(mew_hat)???

                    Var(mew_hat) = SE(mew_hat)^2
                                 = (sigma^2)/n
                                
                                where signma is the std of y
                                 This is the averageg amt mew_hat
                                 differs from mew
            
                                We can do the same for B0_hat, B1_hat
                                    SE(B0_hat)^2 = sigma^2(1/n + (x^2)/(sum(n, i=1, (xi - x)^2))), x = 1/n * sum(n, i=1, xi)
                                    SE(B1_hat)^2 = (sigma^2)/(sum(n, i=1, (xi-x)^2))

                                    where sigma^2 = var(epsilon)

                                    model:
                                        y = B0 + B1*x + epsilon

                                        y - B0 - B1*x = epsilon
                                       \------------v----------/
                                               residuals

                    Typically, sigma^2 has to be estimated by RSE^2, where
                        RSE = sqrt(RSS/(n-2))
                            residual standard error

                        RSS = sum(n, i=0, (yi - (B0_hat+B1_hat*xi))^2)
                            residual sum of squares

                     Note: if z ~ N(mew, sigma^2), then the 95% interval
                      about mew of z is 
                            [mew-25, mew+25]

                      95% confidence intervals for B0_hat and B1_hat are
                        B0_hat: B0_hat +- 2*SE(B0_hat)
                            [B0_hat - 2*SE(B0_hat), B0_hat + 2*SE(B0_hat)]

                        B1_hat: B1_hat +- 2*SE(B1_hat)
                            [B1_hat - 2*SE(B1_hat), B1_hat - 2*SE(B1_hat)]

            
                l
            Hypothesis Testing:
    (null hyp)  H0: there is no relationship between X & Y
     (alt hyp)  Ha: there is a relationship between X & y

                <==>

                H0: B1 = 0
                Ha: B1 not= 0


                In other words, given B1_hat and SE(B1_hat), what is the 
                 probability B1 = 0

                Compute the t statistic
                    assume B1_hat has 0 mean

                    t = (B1_hat - signma) / SE(B1_hat)

                    which is the number of standard errors (SE(B1_hat))
                     is from 0

                    Under some assumptions,
                        t is a t-distribution with n-2 degrees of freedom

                    (idk if this is right) If abs(t) is large, probably null hyp, otherwise probably alt hyp

        advertising example:
            y = sales
            x = tv advertising
            model: y = B0 + B1*x

            least squares to get B0_hat, B1_hat

            Question: does y depend on x?

            H0: B1 = 0, then y does not depend on x
            Ha: b1 not= 0, then y depends on x

            Fit model to data:
                B0_hat, B1_hat

                t0_hat = (B0_hat -sigma)/(SE(B0_hat)) = 15.36
                t1_hat = (B1_hat -sigma)/(SE(B1_hat)) = 17.67
                    p-values for these are basically zero

                    P(t > t_hat) about= 0
                        in both cases, so we can REJECT H0 and
                         conclude B0 not= 0 and B1 not= 0

                In this case, RSE is 3.26 = sqrt(RSS/(n-2))
                    What does this mean????
                        
    ____________________
    / Last time \

    Data: {(xi, yi)}
    model: yi = B0 + B1*xi + epsilon


    Least squares fit:
    (B0_hat, B1_hat) = arg min sum(n, i=1, yi - (B0 + B1*xi))^2

    y = B0_hat + B1_hat*x 

    RSS = sum(n, i=1, yi - (B0_hat + B1_hat*xi))^2

    t-test for whether B0 or B1 = 0
    H0: B1 = 0
    t = B1_hat/(SE(B1_hat))

    if B1 = 0, t is close to 0.
    Moreover, t ~ t distribution
        p-value (probability of tails)

    SE(B0_hat), SE(B1_hat), see notes:
    

    R^2-statistic:  {between 0 & 1}
                    {about= 0 poor fit, about= 1 really good fit}

    R^2 = (TSS - RSS)/TSS
        where TSS is the amount of bariation about the mean
            TSS = sum(nn, i=1, (yi-y_hat)^2)
        
    R^2 = TSS/TSS - RSS/TSS
        = 1 - RSS/TSS  , where RSS < TSS

        In the example (sales)
            sales = B0_hat + B1_hat*TV
            R^2 = 0.61
                implies about 61% of variation in sales is attributed
                to TV advertising


    Alternatively, we could use the correlation:
        
        corr(X, Y) = (sum(n, i=1, (xi - x_hat)(yi - y_hat)))/(sqrt(sum(n, i=1, xi - x_hat)) * sqrt(sum(n, i=1, yi - y_hat)))

        For simple linear regression R^2 = cor(X, Y)

    Multiple Linear Regression
        Y = B0 + B1*X1 + B2*X2 + B3*X3 + epsilon
      sales = B0 + TV  + radio +  news

        Estimate B0, B1, B2, B3


        (B0_hat, B1_hat, B2_hat, B3_hat) =
        = arg min sum(n, i=1, yi - (B0 + B1*X1i + B2*X2i + B3*X3i))^2

        Data: {((x1i, x2i, x3i), yi)}
                  tv  rad  news  sales

        Alternatively
            [1  X11  X21  X31]      [y1]        [B0]
        X = [1  X11  X21  X31], y = [y2],   B = [B1]
            [.  ...  ...  ...]      [..]        [B2]
            [1  X1n  X2n  X3n]      [yn]        [B3]


        B_hat = arg min ||XB - y||^2
              = (X^T * x)^-1 * X^T * y

        Normal equations: (X^T * X)*B_hat = X^T * Y

        Model fit: y_hat = X*B_hat


        In our case B3 has a p-value of 85%, so 
            H0: B3=0 is reasonable, so we fail to reject H0
            
            This suggests we can drop newspaper from the model


        Questions:
            1. Is at least one of X1, ..., Xp useful in predicting Y?
            2. Do all X1, ..., Xp help to explain Y or just a subset?
            3. How does the model fit the data?

            Answers
                1. the f test   

____________________

Last time continued:

    data: {(xi, yi)} , xi = (xi1, xi2, ..., xip)
    Model: Y = B0 + B1*X1 + ... + Bpxp

    fit model to data using least squares   
        B0_hat, B1_hat, ..., Bp_hat = arg min sum(n, i=1, (yo - (B0 + B1*xi1 + ... + Bp*xip))^2)

        ||y - XB||^2

    Question: Is at least one of x1, ..., xp useful in predicting Y?
     if not, Y = B0 is sufficient, ==> Y = y_bar
        <=> B0_hat = arg min sum(n, i=1, yi - B0)^2 ==> B0_hat = 1/n sum(n, i=1, yi) = y_bar

    
    Y = y_bar
       vs.
    Y = B0 + B1*X1 + ... + BpXp


Tests:
    F - test: 
        TSS = sum(n, i=1, yi - y_bar)^2

        RSS = sum(n, i=1, yi - (B0 + B1*X1 + ... + BpXp))^2


    F - statistic:
        F = [(TSS - RSS)/p]/[RSS/(n - p - 1)]
        |
        |--> F-distribution, under reasonable assumptions

    
    ex.
        Let H0: B1 = B2 = ... = Bp = 0
         Then F is an F distribution with p, n - p degrees of freedom

        If H0 is true, then F about= 1
        
        In the advertising example
         F = 570, which have p-value of about= 0.

         Thus, we can reject the H0, implying the linear terms 
          (B1*x1 + ... + BpXp) are important


We can test whether a subset of parameters is 0:
    H0 = Bp - 1 + 1 = Bp - q +z = ... = Bp = 0
        (B1, ..., Bp - q, are nonzero)

    RSS0 = sum(n, i=1, (yi - (B0_hat + B1_hat*Xi1 + ... + Bp-q_hat*Xip-q))^2)
                        least squares fit

    Assuming H0, 
        F = ((RSS0 - RSS)/q)/(RSS/(n - p - 1))

        has an F distribution with q & n - p - 1 degrees of freedom

    For removing one variable, the F-test is the T-test

    The T-values for B1_hat, B2_hat, B3_hat above are
     equivalent to the F-test that omits one variable
        
        So F = t^2 in those cases

    Recall: we failed to reject H0: B3 = 0 (newspaper term)


Note: F-statistic:
        p < n is Typically needed in order to be effective
    
Deciding on important variables
    Variable selection: 

        Model Y = B0 + B1*X1 + ... + Bp*Xp

        Do we need all these terms?

        # variables
            1. B0
            2. B0, B1 ; B0, B2 ; etc
            ..          .......
            p. full model

            2^p models!


        Forward selection:
            Start with Y = B0 and keep adding variables until
             you see no improvement

        Backwards selection:
            Start with full model and remove variables until
             the model doesn't fit the data

        Mixed selection:
            See book

    
Model Fit Measures:

    RSE = sqrt((1/(n - p - 1)) * RSS)

    RSS = sum(n, i=1, (yi - yi_hat))^2

    R^2 = (TSS - RSS)/TSS

    TSS = sum(n, i=1, yi - y_bar)^2


    advertising example:
        R^2 = 0.89720 for full model
            = 0.89719 when B3 = 0 (newspaper dropped)
 
        when B2 = B3 = 0
        R^2 = 0.61 
           |
           v
    adding B2*X2 to the model 
           |
           v
        when B3 = 0
        R^2 = 0.89719

_________________

Predictions:
    {(yi, xi)}^2_i=1
    xi = (x1, ..., xip)

    model: Y = B0 + B1*X1 + ... + BpXp + episilon

    Least squares fit: Y_hat = B0_hat + B1_hat*X1 + ... + Bp_hat*Xp
        approximate model

    1. Reducible error:
        Y_hat vs Y 
            Y_hat = B0_hat + B1_hat*X1 + ... + Bp_hat*Xp
            Y = Y = B0 + B1*X1 + ... + BpXp

        We use confidence intervals to determine how close Y_hat is to Y
        This asssumes we know the true model

    2. Model Bias 
        Y = Y = B0 + B1*X1 + ... + BpXp
            only approximates reality

    3. Irreducible Error
        even if we know the model exactly, we still have epsilon

        How much will Y vary from Y_hat?
        We use prediction intervals to answer this    
            These are always wider than confidence intervals

        Example: 
            $10,000 spent on TV
            $20,000 spent on Radio

            fit: Y_hat = B0_hat + B1_hat*X1 + B2_hat*X2
                 Y_hat = B0_hat + B1_hat*10000 + B2_hat*20000

            95% Confidence interval:
                Y_hat in [10985, 11528]
            
            95% Prediction interval: add the effect of epsilon
                [7930, 14580]

            These both have center 11,256


Qualitative Predictors
    ex. Credit data
        Question: Is there a relationship between credit card debt and
         home ownership?

        (ANOVA or analysis of variance)
        Lets suppose
            yi = credit card balance
            xi = {1} ith person owns
                 {0} otherwise

        model: yi = B0 + B1*X1 + epsiloni

                  = {B0 + B1 + epsiloni}  ith = homeowners
                    {B0 + epsiloni}       ith not a homeowner

        B0 + B1 = mean credit card debt for homeowners
        B0 = mean credi card debt for non homeowners

        [y1]   [1  0][B0]
        [y2] = [1  1][B1]
        [..]   [.  .]
        [y4]   [1  1]

        B0_hat: 509.8, SE = 33.17, t = 15.37, p about= 0
        B1_hat: 19.7, SE = 46.1, t = 0.42, p = 0.67

        If H0: B1 = 0, we cannot reject H0 since p value = 0.67
        If H0: B0 = 0, we can reject the null hyp since p about= 0

    
    Qualitative with more than two levels
        xi1 = {1}     ith person from South
              {0}     not
        
        xi2 = {1}     ith person from West
              {0}     not

        yi = B0 + B1*X1 + B2_X2

           = {B0 + B1}   ith person from South
             {B0 + B2}   ith person from West
             {B0}        ith person from East

        SE(B1_hat): 531, -12.5, -18.7
        t-values: 11.5, -0.22, -0.29
        p-values: 0, 0.83, 0.77

        so cannot reject B1 = 0, or reject B2 = 0           


    Back to advertising
        a 1 unit increase in X, yields B1 incrase in Y. Similarly for
         X2.

        Question: do X1 and X2 interact?

        Consider Y = B0 + B1*X1 + B2*X2 + B3*X1*X2 + epsilon
                   
                   = B0 + (B1+B3*X3)*X1 + B2*X2 + epsilon 

        
        Fit this model to data:
            R^2 = 0.968   (0.897 for (*))
            p-values
                B0_hat: 0
                B1_hat: 0
                B2_hat: 0.001
                B3_hat: 0

        for all cases, we can reject
            H0: bj = 0,    j = 0, 1, 2, 3

____________________

Nonlinearity in data
    transfer x (add polynomial terms)
    use sqrt(x), log(x)
    transform y: sqrt(y), log(y)


Outliers:
    values of yi far from the model

Leverage Points:
    unusual value for xi, that have a large impact on the regression linear

    Levelargej = 1/n + (xj0x_bar)/(sum(n,i=1, (xi-x_bar)^2))

Colinearity:
    Credit['limit'] and Credit['age']
        no obvious correlation

    Crdit['limit'] and Credit['rating'] are Colinearity

Problem?
    Variance in flation factor

        = VIF(B0_hat) = 1/(1-R^2_(xj|x-j))
    
        where R^2_(xj|x-j) is the R^2 of regressing line of xj or remain variables

    High Correlation ==> VIF(Bj_hat) is large 

    *VIF(Bj_hat) > 5 or 10, there is high colinearity